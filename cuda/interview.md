# 1

## 整体情况简介

面试中的问题基本上分成以下几类：

1. 基础的八股文：C/C++，OS，计算机体系结构等。这一部分略，网上已经有很多总结了。
2. 高性能计算基础知识：这一部分是面试的重点，本文章以CUDA为重点。
3. 各种AI框架知识：本文章以推理方向为主。
4. AI基础知识：对于常见的机器学习算法，以及CV & NLP & 推荐模型有一定了解，了解计算流程以及模型结构即可，重点为了能分析出计算瓶颈在哪里，找出可能优化的方向。本部分略
5. 算法题： 手写CUDA kernel和leetcode的比例大约为3:1。手写CUDA kernel的时候一般会结合第2部分一起问，一步一步要求你优化，每一步优化的具体原理，涉及到什么硬件知识等。

## 高性能计算基础

1. CUDA的线程组织结构
2. CUDA的存储体系结构，每一种存储的优缺点，该如何合理使用。
3. GPU每一代的新特性有了解过吗？应该从哪里去了解详细信息？
4. CUDA stream的概念，为什么要使用多个stream？
5. GPU和CPU分别适合执行哪些程序？结合它们的硬件架构解释一下为什么它们有各自的优势。
6. 说明一下神经网络加速器与CPU、GPU的区别，他们各自有何优势？
7. 半精度浮点数FP16各个部分的具体位数，为什么要有半精度浮点数？
8. TensorCore的加速原理
9. MPI，OpenMP以及CUDA各自适用的加速场景。
10. RDMA相关问题。
11. 平时如何进行kernel的优化，会用到哪些工具？
12. CPU上哪些并行优化方法？
13. ARM相关的库有了解过吗？
14. PTX有了解过吗？
15. roofline模型有什么用？如何确定最优的BLOCK_SIZE。
16. GPU资源调度有哪些方法？
17. 稀疏矩阵的存储格式有哪些？稀疏矩阵的应用场景？稀疏矩阵计算与稠密矩阵计算有何不同？
18. 如何计算CPU指令的吞吐量和时延?

## AI 框架知识

这一部分会涉及一些AI框架(训练&推理&编译器)相关的问题，并且会重点根据简历上的项目经历去做一些发散性的提问。

1. MLIR有了解过吗？ONNX有了解过吗？
2. TVM的整体结构，如何用TVM进行开发？
3. 为什么要进行推理优化？直接用tensorflow或者pytorch的推理接口不行吗？
4. 模型推理优化的常用方法有哪些？
5. 有研究过某一个框架的具体源码吗？
6. TensorRT如何进行自定义算子开发？
7. TensorRT对模型实现了哪些推理优化？常量折叠，算子融合，量化....
8. 算子融合为什么能加速推理，优化了哪一部分？TensorRT用到了哪些算子融合？算子融合在推理框架中是如何实现的？
9. 模型量化的加速原理，模型量化带来的精度损失如何解决？
10. ONNX Runtime支持在多种硬件上进行推理，说明具体的实现机制。
11. 总结一下TensorRT，ONNX Runtime等推理框架的组成架构，如果我们公司自己要为硬件开发一套推理框架，应该重点关注哪些部分？
12. 各种推理框架都有何优劣势？它们的性能怎么样？
13. 分布式训练中有哪些并行模式？每种模式需要做什么，有什么优缺点？
14. 分布式训练中我们重点需要处理的问题有哪些？目前已有哪些解决方案
15. MPI如何应用于AI框架中？
16. 模型在移动端进行推理优化的框架有了解过吗？移动端和在服务器的推理优化思路有何不同？移动端能用到的加速指令有了解过吗？
17. 移动端有哪些加速方法？
18. 为什么要将模型一部分推理优化放在移动端，全部放在服务器上不可以吗？
19. 自动驾驶上的推理框架有了解过吗？我们重点需要关注的指标有哪些？
20. 反向传播的原理，具体实现的源码有了解过吗？
21. 你了解哪些推理模型的调度方法？
22. 推荐模型的结构有了解过吗？要部署一个大的推荐模型，应该如何将各个部分放在哪种硬件上部署？
23. 计算图切分有了解过吗？如何应用于大模型推理？
24. TensorFlow和Pytorch都用过吗？它们设计思路有何不同？有何优劣？如何添加自定义算子？

## 算法题

手写CUDA kernel几乎每场面试都会考，面试官会以写出来的第一个版本为准，一步步问继续优化的方法，在这个期间会结合高性能计算的基础知识来考察，从这个过程中能了解到对体系结构以及优化方法的了解程度。leetcode不一定有，但是遇上了基本上都是hard。两类算法题都要准备。

下面是常见的一些问题：

1. 矩阵乘:
2. 矩阵转置: 访存密集型算子的处理
3. 一维reduce-sum：重点是如何处理bank confict
4. 二维reduce-sum
5. 卷积
6. 将单stream改成多stream

以矩阵乘法为例说明一下一个典型的面试流程，下面以A表示面试官，B表示面试者。

A：写一个矩阵乘法吧，并将main函数中具体调用给写清

B: （写了一个最naive版本的矩阵乘）

A: 目前这个程序有什么问题，能进一步优化吗？

B : 目前访存性能比较低，可以采用矩阵分块并且使用上shared memory优化，并解释一下这样做的原理。

A：可以具体计算一下优化前后的计算访存比，来具体说明这一部分提升了多少。并写一下优化后的程序。

B: 通过计算优化了.....

上述对话会重复几轮，在后面几轮可能面试官不会再要求将每一版程序都写出来了，重点在于讨论优化思路，并且在讨论的过程中发散地问一点CUDA的知识考察理解的深度。

## 一些比较零碎的问题

1. 卷积的三种加速计算方式，im2col+GEMM & Winograd & FFT，各自有何优缺点，cuDNN中有哪些实现？
2. 数字信号的采样定理、熵 & 交叉熵 的含义 & 计算公式
3. 还记得KKT条件吗？写程序求解一个非线性方程，并说明具体用到的优化方法。
4. 脑洞问题：如何从编码的角度进行模型压缩？
5. 如何将你研究生阶段的成果应用到我们的产品中？
6. 给了一个TF 模型的profile，找出里面的bottle neck，提出如何改进这个模型的性能的方法。
7. MIPS流水线有几级？分别是哪些组成部分？
8. 说一下transformer的具体结构，如何加速transformer进行推理？
9. attention的计算公式，写一下tf里面对应的代码
10. 马尔科夫链简单知识
11. 一道较难的概率题
12. BN 层有什么用，具体怎么算的？
13. softmax有什么用，怎么做的

## 推荐参考资料

1. 《通用图形处理器设计:GPGPU编程模型与架构原理》：CUDA、GPU体系结构、PTX、TensorCore等GPU知识大杂烩，CUDA相关面试问题标答。对于GPU的硬件体系结构有较深入的介绍，虽然比较难懂，但是这一部读完后会对CUDA编程模型以及为什么要采用一些特定的优化方法有更深入的理解。
2. 官方文档《CUDA Programming Guide》 & 《CUDA Best Practice Guide》: 不解释，必读。
3. 《大规模并行处理器程序设计》：入门最佳，没有之一。其中第二部分对于CUDA中常见的计算Pattern做了分析，几乎可以应付所有的面试中的kernel编程，至少能答出80%，至于更深入地优化方法需要再花时间去研究。
4. 《机器学习系统：设计和实现》：介绍了ML Sys这一领域的所有方面的基础知识，可以从一个整体的层面对机器学习系统的组成部分、每个部分的重点技术有较好的把握。这本书的框架主要以MindSpore为例，所以在整体读完后，需要结合自己比较熟悉的框架进一步仔细理解。该书有在线版本[机器学习系统：设计和实现](https://openmlsys.github.io/)
5. 《深度学习进阶：自然语言处理》：只用numpy实现NLP模型，可以作为阅读深度学习框架源码的first course，会对AI模型中的底层实现细节有很好的理解。
6. 《分布式机器学习：理论、算法与实践》：可以对分布式训练有大致的了解
7. 《AI编译器开发指南》：深度学习编译器相关的介绍，重点在TVM。

建议： 1 ~ 4必读，这是所有领域的基础知识，5 ~ 7需要根据个人的研究兴趣和方向有选择性地深入阅读。

# 2

【面试】3技术面 1hr面
【时间】23.08
【公司】蔚来
【岗位】自动驾驶高性能计算
【面经】
1.c++11 14 17 20八股基本问了个遍
2.leetcode3道mid一道hard
3.cuda、tensorRT、TVM相关

作者：浮华梦～
链接：[https://www.nowcoder.com/discuss/521847753123635200?anchorPoint=comment](https://www.nowcoder.com/discuss/521847753123635200?anchorPoint=comment)
来源：牛客网

# 小抄

## 通用优化

避免 global mem 的重复访问

合理使用原子操作 https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/

避免warp分支

减少数据传输

合并访存

双缓存技术

sharedmem 避免 banck conflect （跨步访问，空一错开）

面向warp编程

gridsize 参考 SM 数量

### 优化内存与显存传输效率

* 使用Pinned(page-locked) Memory提高传输速度
* 通过在不同的Stream里同时分别执行kernel调用及数据传输，使数据传输与运算并行。（注意default stream的坑 ^[[1]](https://zhuanlan.zhihu.com/p/570795544#ref_1)^ ）
* 尽量将小的数据在GPU端合成大块数据后传输
* 有些情况下，即使数据不太适合使用kernel处理，但如果为了较低的算法latency，也可权衡传输代价后使用kernel处理数据
* 注意PCI-e插口的通道个数

### 优化Kernel访存效率

* **提高Global Memory访存效率**

1. 对Global Memory的访存需要注意合并访存(coalesced )。^[[2]](https://zhuanlan.zhihu.com/p/570795544#ref_2)^
2. warp的访存合并后，起始地址及访存大小对齐到32字节
3. 尽量避免跨步访存
4. 8.0及以上的设备可以通过编程控制L2的访存策略提高L2命中率。

* **提高Shared Memory的访存效率**

1. shared memory由32个bank组成
2. 每个bank每时钟周期的带宽为4字节
3. 连续的4字节单元映射到连续的bank。如0-3字节在bank0，4-7字节在bank1……字节128-131字节在bank0
4. 若warp中不同的线程访问相同的bank，则会发生bank冲突(bank conflict)，bank冲突时，warp的一条访存指令会被拆分为n条不冲突的访存请求，降低shared memory的有效带宽。所以需要尽量避免bank冲突。
5. CUDA 11.0以上可以使用*async-copy* feature^[[3]](https://zhuanlan.zhihu.com/p/570795544#ref_3)^

### 优化线程级并行

occupancy calculator

### 指令级优化

* **提高计算访存比**

GPU执行计算时，需要LDS、LDG等指令先将数据读入寄存器，再进行计算，最后通过STS、STG等指令将数据保存下来。

以矩阵乘法为例，先进行矩阵分块，最终拆解为每个线程计算MxK,KxN的两个小矩阵的乘法：

若两小矩阵为M=2,N=2,K=1，即2x1;1x2,最后得到2x2的矩阵作为结果。则读入4个float需4条指令，计算指令也是4条，计算访存比4/4=1；

若两小矩阵为M=8,N=8,K=1，即8x1;1x8,最后得到8x8的矩阵作为结果。则读入16个float，需读取指令16条，计算指令8x8=64条，计算访存比64/16=4；若使用向量读(float4)每条指令读入4个float，则读取指令仅4条，计算访存比64/4=16

提高计算访存比，可以让GPU的更多时钟周期用于进行计算，相对的进行数据IO占用的时钟周期更少。

### 指令级优化

* **提高计算访存比**

GPU执行计算时，需要LDS、LDG等指令先将数据读入寄存器，再进行计算，最后通过STS、STG等指令将数据保存下来。

以矩阵乘法为例，先进行矩阵分块，最终拆解为每个线程计算MxK,KxN的两个小矩阵的乘法：

若两小矩阵为M=2,N=2,K=1，即2x1;1x2,最后得到2x2的矩阵作为结果。则读入4个float需4条指令，计算指令也是4条，计算访存比4/4=1；

若两小矩阵为M=8,N=8,K=1，即8x1;1x8,最后得到8x8的矩阵作为结果。则读入16个float，需读取指令16条，计算指令8x8=64条，计算访存比64/16=4；若使用向量读(float4)每条指令读入4个float，则读取指令仅4条，计算访存比64/4=16

提高计算访存比，可以让GPU的更多时钟周期用于进行计算，相对的进行数据IO占用的时钟周期更少。

* **提高指令级并行**

指令级并行基本原理：

* 现代不论是CPU还是GPU，指令的执行都是通过流水线进行的，流水线分为多个stage，即一条指令执行完成需要每个stage的工作都执行完成。而一个时钟周期并不是完成一条指令执行的所有时间，而是每一个stage完成当前工作的时间。流水线可以同时执行多条指令的不同阶段。
* 当后续指令的执行需要依赖前面指令的结果写回寄存器，我们说出现了寄存器依赖。此时后续指令需要等待第前面指令结果写回寄存器才能执行，若后续指令执行时前面指令结果尚未写回寄存器，流水线会失速（stall），此时warp scheduler开始切换到其它eligible warp，若无eligible warp，则SMSP将会空转。
* 若后续指令不依赖前面指令的结果，则即使前面指令未执行完毕，后续指令也可以开始执行。特别的，即使前序指令是一条耗时几百周期的LDG(全局内存读取)指令或耗时几十周期的LDS（共享内存读取）指令，只要后续一系列指令不依赖读取回来的数据，后续一系列指令可以正常执行而不必等待该LDG/LDS指令执写回寄存器。

通过以下方式，可以提高指令级并行，在线程级并行达不到较好效果的情况下，进一步提高程序性能：

* 数据预取（Prefetch）：数据1已读取到寄存器，使用该数据1计算前，先将后续数据2的读取指令发射，再执行一系列数据1的处理指令；这样数据1的处理和数据2的读取在流水线上同时执行着。当数据1处理完成，需要处理数据2时，可以确保数据2已经存在于寄存器中，此时类似的将数据3的读取和数据2的处理同步执行起来。
* 指令重排：在存在寄存器依赖的指令间插入足够的其它指令，使得后续指令执行时，前面计算指令的结果已写回到寄存器。从CUDA C层面有意识地提供一些语句间的并行性，nvcc编译器可以一定程度上自动进行指令重排。若对nvcc重排结果不满意需要自己重排时，官方尚未开放SASS汇编器，目前只存在一些第三方SASS汇编器工具 ^[[5]](https://zhuanlan.zhihu.com/p/570795544#ref_5)^ 。
* 
* **提高Register的效率**

1. Register File也存在bank冲突，但在CUDA C层面上没有直接办法进行物理寄存器控制。
2. 可以通过SASS汇编器，人工进行指令寄存器分配，以尽量消除register bank conflict。
3. 可以通过SASS汇编器，为寄存器访问添加reuse标记，以尽量消除register bank conflict。
4. 

### 使用TensorCore进一步加速矩阵运算^[[6]](https://zhuanlan.zhihu.com/p/570795544#ref_6)^

TensorCore可以用来快速进行D=A*B+C矩阵运算，提供 `load_matrix_sync`， `store_matrix_sync`， `mma_sync` 等API。

## reduce 优化：

分支发散：tread 集中

banck 冲突： thread 连续访问 bank

初始化闲置：load and add

最后一次循环不需要同步和条件判断：展开

完全展开

一个线程计算更多，lunch开销，

https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/

# 延伸阅读

CUB

https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/

https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/

https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/
